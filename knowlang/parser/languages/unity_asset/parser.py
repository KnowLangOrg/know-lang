from pathlib import Path
import os
from typing import List, Union
from dataclasses import dataclass

from knowlang.core.types import BaseChunkType, CodeChunk, CodeLocation, LanguageEnum
from knowlang.parser.base.parser import LanguageParser
from knowlang.utils import FancyLogger
import aiofiles
import json

LOG = FancyLogger(__name__)


@dataclass
class Node:
    x: float
    y: float
    id: str
    type: str
    guid: str
    raw_json: str
    graph_id: int  # generated by us


@dataclass
class Connection:
    start_id: str
    end_id: str
    id: str
    type: str
    raw_json: str
    graph_id: int  # generated by us


@dataclass
class Group:
    x_min: float
    y_min: float
    x_max: float
    y_max: float
    id: str
    type: str
    raw_json: str
    graph_id: int  # generated by us


class UnityAssetParser(LanguageParser):
    """Parser for Unity Assets"""

    def setup(self) -> None:
        """Initialize parser for Unity Assets"""
        self.language_name = LanguageEnum.UNITYASSET
        self.language_config = self.config.languages[LanguageEnum.UNITYASSET.value]

    def _parse_element(self, raw_element: dict) -> Union[Node, Connection, Group]:
        """Parse a raw element dictionary into the appropriate data class"""
        element_type = raw_element.get("$type", "")

        if "Connection" in element_type:
            # Parse connection (both control and value connections)
            return Connection(
                start_id=raw_element.get("sourceUnit", {}).get("$ref", ""),
                end_id=raw_element.get("destinationUnit", {}).get("$ref", ""),
                id=raw_element.get("guid", ""),
                type=element_type,
                raw_json=json.dumps(raw_element),
                graph_id=0,
            )
        elif "GraphGroup" in element_type:
            # Parse group
            position = raw_element.get("position", {})
            return Group(
                x_min=position.get("xMin", 0.0),
                y_min=position.get("yMin", 0.0),
                x_max=position.get("xMax", 0.0),
                y_max=position.get("yMax", 0.0),
                id=raw_element.get("guid", ""),
                raw_json=json.dumps(raw_element),
                type=element_type,
                graph_id=0,
            )
        else:
            # Parse node (everything else with an ID)
            position = raw_element.get("position", {})
            return Node(
                x=position.get("x", 0.0),
                y=position.get("y", 0.0),
                id=raw_element.get("$id", ""),
                raw_json=json.dumps(raw_element),
                type=element_type,
                guid=raw_element.get("guid", ""),
                graph_id=0,
            )

    async def parse_file(self, file_path: Path) -> List[CodeChunk]:
        """Parse a single Unity Asset file and return list of parsed elements from the visual script"""
        if not self.supports_extension(file_path.suffix):
            LOG.debug(f"Skipping file {file_path}: unsupported extension")
            return []

        # Check file size limit
        if file_path.stat().st_size > self.language_config.max_file_size:
            LOG.warning(
                f"Skipping file {file_path}: exceeds size limit of {self.language_config.max_file_size} bytes"
            )
            return []

        async with aiofiles.open(file_path, "r") as f:
            source_code = await f.read()

        # Parse Unity asset file and extract Visual Script JSON
        json_content = ""
        in_json = False

        for line in source_code.split("\n"):
            if "_json: '" in line:
                # Start of JSON, extract the part after the quote
                json_start = line.find("_json: '") + len("_json: '")
                json_content = line[json_start:]
                in_json = True
            elif in_json:
                # Continue collecting JSON until we find the closing quote
                json_content += line

            # Check if we've reached the end of JSON
            if in_json and line.strip().endswith("'"):
                # Remove the trailing quote and any leading whitespace
                json_content = json_content.rstrip("'\n\r ")
                break

        # Clean up the JSON - remove YAML formatting
        json_content = json_content.replace("\n      ", "")  # Remove YAML indentation
        json_content = json_content.replace('""', '"')  # Fix double quotes

        # Check to make sure we have JSON content
        if len(json_content) == 0:
            LOG.warning(f"Skipping file {file_path}: No JSON content found")
            return []

        try:
            graph_data = json.loads(json_content)
        except json.JSONDecodeError as e:
            LOG.warning(f"Skipping file {file_path}: JSON content not parseable")
            return []

        # Extract all raw elements from the graph
        raw_elements = graph_data.get("graph", {}).get("elements", [])

        # Parse each raw element into the appropriate data class
        nodes = {}
        connections = []
        groups = []
        for i, raw_element in enumerate(raw_elements):
            element = self._parse_element(raw_element)
            element.graph_id = i
            if isinstance(element, Node):
                nodes[element.id] = element
            elif isinstance(element, Connection):
                connections.append(element)
            elif isinstance(element, Group):
                groups.append(element)

        def merge_elements(elements):
            if len(elements) == 0:
                return
            shared_graph_id = elements[0].graph_id
            for element in elements:
                element.graph_id = shared_graph_id

        # Create connected graphs based on groups first
        for group in groups:
            group_nodes = []
            for node in nodes.values():
                x_collide = group.x_min <= node.x <= group.x_max
                y_collide = group.y_min <= node.y <= group.y_max
                if x_collide and y_collide:
                    group_nodes.append(node)

            merge_elements(group_nodes + [group])

        def merge_graphs(graph_ids):
            if len(graph_ids) == 0:
                return
            new_graph_id = graph_ids[0]
            all_elements = list(nodes.values()) + connections + groups
            elements_to_merge = list(
                filter(lambda e: e.graph_id in graph_ids, all_elements)
            )
            for element in elements_to_merge:
                element.graph_id = new_graph_id

        for connection in connections:
            if not (connection.start_id in nodes and connection.end_id in nodes):
                continue
            start_node_graph_id = nodes[connection.start_id].graph_id
            end_node_graph_id = nodes[connection.end_id].graph_id
            merge_graphs([start_node_graph_id, end_node_graph_id, connection.graph_id])

        elements_by_graph = {}

        for element in list(nodes.values()) + connections + groups:
            if not element.graph_id in elements_by_graph:
                elements_by_graph[element.graph_id] = []
            elements_by_graph[element.graph_id].append(element)

        chunks = []

        relative_path = os.path.relpath(file_path, self.config.directory_path)

        for i, graph_elements in enumerate(elements_by_graph.values()):
            raw_json_elements = map(lambda e: e.raw_json, graph_elements)
            content = "[" + (", ").join(raw_json_elements) + "]"

            chunk = CodeChunk(
                language=self.language_name,
                type=BaseChunkType.OTHER,
                name="",
                content=content,
                location=CodeLocation(
                    file_path=str(relative_path),
                    start_line=i,
                    end_line=i + 1,
                ),
                # TODO: implement docstring based on the yaml's metadata
                docstring="",
            )
            chunks.append(chunk)

        return chunks

    def supports_extension(self, ext: str) -> bool:
        """Check if this parser supports a given file extension"""
        return ext in self.language_config.file_extensions
